# Stochastic optimization

- **On the importance of initialization and momentum in deep learning** <br />
  Ilya Sutskever, James Martens, George Dahl, Geoffrey Hinton <br />
  _ICML_ (2013) <br />
  https://proceedings.mlr.press/v28/sutskever13.html
  
- **Adam: A Method for Stochastic Optimization** <br />
  Diederik P. Kingma, Jimmy Ba <br />
  _ICLR_ (2015) <br />
  https://arxiv.org/abs/1412.6980

- **Incorporating Nesterov Momentum into Adam** (NAdam) <br />
  Timothy Dozat <br />
  _ICLR_ (2016) <br />
  https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ

- **On the Convergence of Adam and Beyond** <br />
  Sashank J. Reddi, Satyen Kale, Sanjiv Kumar <br />
  _ICLR_ (2018) <br />
  https://arxiv.org/abs/1904.09237

- **Decoupled Weight Decay Regularization** (AdamW) <br />
  Ilya Loshchilov, Frank Hutter <br />
  _ICLR_ (2019) <br />
  https://arxiv.org/abs/1711.05101

- **On the variance of the adaptive learning rate and beyond** (RAdam) <br />
  Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han <br />
  _ICLR_ (2020) <br />
  https://arxiv.org/abs/1908.03265

