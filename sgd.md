# Stochastic optimization

## References

- **The energy landscape of a simple neural network** <br />
  Gamst A, Walker A <br />
  _OPTML_ @ NeurIPS (2017) <br />
  http://opt-ml.org/oldopt/papers/OPT2017_paper_1.pdf

- **Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs** <br />
  Garipov T, Izmailov P, Podoprikhin D, Vetrov D, Gordon Wilson A <br />
  _NeurIPS_ (2018) <br />
  https://arxiv.org/abs/1802.10026

- **Visualizing the Loss Landscape of Neural Nets** <br />
  Li H, Xu Z, Taylor G, Studer C, Goldstein T <br />
  _NeurIPS_ (2018) <br />
  https://arxiv.org/abs/1712.09913

- **Shaping the learning landscape in neural networks around wide flat minima** <br />
  Baldassi C, Pittorino F, Zecchina R <br />
  _PNAS_ (2019) <br />
  https://doi.org/10.1073/pnas.1908636117

- **Loss Landscapes of Regularized Linear Autoencoders** <br />
  Kunin D, Bloom J, Goeva A, Seed C <br />
  _ICML_ (2019) <br />
  https://proceedings.mlr.press/v97/kunin19a.html

- **Deep learning versus kernel learning:
  an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel** <br />
  Fort S, _et al._ <br />
  _NeurIPS_ (2020) <br/ >
  https://arxiv.org/abs/2010.15110

- **On the Loss Landscape of Adversarial Training:
  Identifying Challenges and How to Overcome Them** <br/ >
  Liu C, Salzmann M, Lin T, Tomioka R, Süsstrunk S <br />
  _NeurIPS_ (2020) <br />
  https://papers.nips.cc/paper/2020/file/f56d8183992b6c54c92c16a8519a6e2b-Paper.pdf

- **Online and Stochastic Optimization beyond Lipschitz Continuity: A Riemannian Approach** <br />
  Antonakopoulos K, Belmega EV, Mertikopoulos P <br />
  _ICLR_ (2020) <br />
  https://openreview.net/forum?id=rkxZyaNtwB

- **Exploiting Uncertainty of Loss Landscape for Stochastic Optimization** <br />
  Bhaskara VS, Desai S <br />
  _Preprint_ (2019) <br />
  https://arxiv.org/abs/1905.13200

- **Stochasticity helps to navigate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval problem** <br />
  Mignacco F, UrbaniP, Zdeborová L <br/ >
  _Mach Learn: Sci Technol_ **2**, 035029 (2021) <br />
  https://doi.org/10.1088/2632-2153/ac0615

- **Landscape and training regimes in deep learning** <br />
  Geiger M, Petrini L, Wyart M <br />
  _Physics Reports_ **924**, 1-18 (2021) <br />
  https://doi.org/10.1016/j.physrep.2021.04.001

- **Learn2Hop: Learned Optimization on Rough Landscapes** <br />
  Merchant A, Metz L, Schoenholz SS, Cubuk ED <br />
  _ICML_ (2021) <br />
  https://proceedings.mlr.press/v139/merchant21a.html

- **Tilting the playing field: Dynamical loss functions for machine learning** <br />
  Ruiz-Garcia M, Zhang G, Schoenholz SS, Liu AJ <br />
  _ICML_ (2021) <br />
  https://proceedings.mlr.press/v139/ruiz-garcia21a

- **Embedding Principle of Loss Landscape of Deep Neural Networks** <br />
  Zhang Y, Zhang Z, Luo L, Xu Z <br />
  _NeurIPS_ (2021) <br />
  https://papers.nips.cc/paper/2021/hash/7cc532d783a7461f227a5da8ea80bfe1-Abstract.html

- **Loss landscapes and optimization in over-parameterized non-linear systems and neural networks** <br />
  Liu C, Zhu L, Belkin M <br />
  _Applied and Computational Harmonic Analysis_ **59**, 85-116 (2022) <br />
  https://doi.org/10.1016/j.acha.2021.12.009

- **On the Implicit Bias in Deep-Learning Algorithms** <br />
  Vardi G <br />
  _Comm ACM_ **66**, 86-93 (2023) <br />
  https://doi.org/10.1145/3571070

- **Loss Landscapes are All You Need: Neural Network Generalization Can Be Explained Without the Implicit Bias of Gradient Descent** <br />
  Chiang PY, Ni R, Miller DY, Bansal A, Geiping J, Goldblum M, Goldstein T <br />
  _ICLR_ (2023) <br />
  https://openreview.net/forum?id=QC10RmRbZy9

### Papers to sort

- **Learning Functions: When Is Deep Better Than Shallow** <br />
  Hrushikesh Mhaskar, Qianli Liao, Tomaso Poggio <br />
  https://arxiv.org/abs/1603.00988

- **Deep vs. shallow networks : An approximation theory perspective** <br />
  Hrushikesh Mhaskar, Tomaso Poggio <br />
  https://arxiv.org/abs/1608.03287

- **Why and When Can Deep -- but Not Shallow -- Networks Avoid the Curse of Dimensionality: a Review** <br />
  Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, Qianli Liao <br />
  https://arxiv.org/abs/1611.00740

- **Theory II: Landscape of the Empirical Risk in Deep Learning** <br />
  Qianli Liao, Tomaso Poggio <br />
  https://arxiv.org/abs/1703.09833

- **Theory of Deep Learning III: explaining the non-overfitting puzzle** <br />
  Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix, Jack Hidary, Hrushikesh Mhaskar <br />
  https://arxiv.org/abs/1801.00173

- **Theory of Deep Learning IIb: Optimization Properties of SGD** <br />
  Chiyuan Zhang, Qianli Liao, Alexander Rakhlin, Brando Miranda, Noah Golowich, Tomaso Poggio <br />
  https://arxiv.org/abs/1801.02254

- **Theory IIIb: Generalization in Deep Networks** <br />
  Tomaso Poggio, Qianli Liao, Brando Miranda, Andrzej Banburski, Xavier Boix, Jack Hidary <br />
  https://arxiv.org/abs/1806.11379

- **Double descent in the condition number** <br />
  Tomaso Poggio, Gil Kur, Andrzej Banburski <br />
  https://arxiv.org/abs/1912.06190

- **Explicit regularization and implicit bias in deep network classifiers trained with the square loss** <br />
  Tomaso Poggio, Qianli Liao <br />
  https://arxiv.org/abs/2101.00072

- **SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks** <br />
  Tomer Galanti, Zachary S. Siegel, Aparna Gupte, Tomaso Poggio <br />
  https://arxiv.org/abs/2206.05794
